{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Date: 04/06/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook will generate three txt files for further processing and Machine Learning.\n",
    "- Input files : \n",
    "  - data.txt that contains the text of 5000 different advertisements \n",
    "  - stopwords.txt contains the stopwords in english that will be used to process the data file\n",
    "\n",
    "- output files :\n",
    "  - vocab.txt: \n",
    "  It contains \n",
    "  the unigram vocabulary in the following format:word_string:integer_index. Words in the vocabulary will be sorted in        alphabetical order. This file is the key to interpret the sparse encoding.\n",
    "             \n",
    "  - sparse.txt: \n",
    "  Each line of this file corresponds to one advertisement. So, they start with advertisement ID. The rest of each line is the sparse representation of the corresponding description in the form of word_index:word_freq separated by a comma. The order of the lines will match the order of the advertisements in the input file.\n",
    "             \n",
    "  - highFreq.txt & lowFreq.txt:\n",
    "  These two files contain frequent words that appear in more than 100 advertisement descriptions as well as the word              appear only once in the whole job advertisement description. Each line contains only one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.probability import *\n",
    "import time\n",
    "import re\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time. time()\n",
    "# Read data from file\n",
    "File=open('./data.txt')\n",
    "Raw_text=File.read().lower().splitlines()\n",
    "\n",
    "# Filtering the ID and description into sepreate list\n",
    "filter_ID = re.compile(r\"^id: (#\\d{8})\")\n",
    "filter_Descr = re.compile(r\"^description: (.+)\")\n",
    "\n",
    "ID = list(filter(filter_ID.match, Raw_text)) \n",
    "Description = list(filter(filter_Descr.match, Raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting the words 'id' and 'description' at the beginning of each element to get the tidy data to work for\n",
    "ID_list=[x[4:] for x in ID]\n",
    "Description_list = [y[13:] for y in Description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the tokenizer and do the word tokenization for the description in each job number\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "tokens = [tokenizer.tokenize(text_descr) for text_descr in Description_list]\n",
    "tokens_dict=dict(zip(ID_list,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the word that length is shorter than 3\n",
    "for k, v in tokens_dict.items():\n",
    "    tokens_dict[k] = [word for word in v if len(word)>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopwords\n",
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a word remove function\n",
    "def exclude_words(target,wordset):\n",
    "    for k,v in target.items():\n",
    "        target[k] = [word for word in v if word not in wordset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords from our token\n",
    "# Using set will increase the process speed\n",
    "stopwordsSet = set(stopwords)\n",
    "\n",
    "exclude_words(tokens_dict,stopwordsSet)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Chain the token set with and without the set method to get two different word sets\n",
    "\n",
    "words_set_1 = list(chain.from_iterable(tokens_dict.values()))\n",
    "words_set_2 = list(chain.from_iterable([set(value) for value in tokens_dict.values()]))\n",
    "\n",
    "fd_1 = FreqDist(words_set_1)\n",
    "fd_2 = FreqDist(words_set_2)\n",
    "# Find the list of low frequency words and high frequency words\n",
    "LessFreqWords = list(fd_1.hapaxes())\n",
    "HighFreqWords = list([k for k, v in fd_2.items() if v > 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove high frequency words\n",
    "HighFreqSet = set(HighFreqWords)\n",
    "    \n",
    "exclude_words(tokens_dict,HighFreqSet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low freqyency words\n",
    "LessFreqSet = set(LessFreqWords)\n",
    "\n",
    "exclude_words(tokens_dict,LessFreqSet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function getfreq to find the number of frequence in each unigram\n",
    "def getfreq (target, wordset):\n",
    "    return list([v for k, v in target.items() if k in wordset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dict of High frequency words\n",
    "HighFreqnum = getfreq(fd_2, HighFreqSet)\n",
    "HighFreqout = dict(zip(HighFreqWords,HighFreqnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output highFreq text with frequency number sorted\n",
    "with open('highFreq.txt', 'w') as f:\n",
    "    high = [(k, HighFreqout[k]) for k in sorted(HighFreqout, key=HighFreqout.get, reverse=True)]\n",
    "    for k in high:\n",
    "        i=k[0]\n",
    "        j=k[1]\n",
    "        print(i+':'+str(j), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output lowFreq text\n",
    "with open('lowFreq.txt', 'w') as f:\n",
    "    LowFreqword = list(LessFreqSet)\n",
    "    for i in sorted(LowFreqword):\n",
    "        print(str(i), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(sorted(chain.from_iterable(tokens_dict.values())))\n",
    "# Output the vocab file\n",
    "out_file = open(\"vocab.txt\", 'w')\n",
    "# Obtain each word from all job description, then assign each word with a index\n",
    "vocab_dict={}\n",
    "i=-1\n",
    "for d in sorted(vocab):\n",
    "    vocab_dict[d] = i\n",
    "    i=i+1\n",
    "    out_file.write(d+':'+str(i)+'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output sparse file\n",
    "\n",
    "out_file = open(\"sparse.txt\", 'w')\n",
    "\n",
    "\n",
    "# create the loop to find the occurrences of each vocab in each ad.\n",
    "for n,d in sorted(tokens_dict.items()):\n",
    "    d_idx = [vocab_dict[w] for w in d]\n",
    "    out_file.write(n+',')\n",
    "    for k, v in sorted(FreqDist(d_idx).items()):\n",
    "        out_file.write(\" {}:{},\".format(k+1,v))\n",
    "    out_file.write('\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time. time()\n",
    "print('This program takes:' + str(end - start)+ 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
