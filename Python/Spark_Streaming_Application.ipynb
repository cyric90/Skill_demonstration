{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming_Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Spark streaming application will:\n",
    "1. receive the streaming data from all three producers\n",
    "2. process the information with newly created datetime and Geohash, then form the data model\n",
    "3. pass the data to MongoDB\n",
    "\n",
    "        Window length = 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "# libraries \n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash as gh\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendDataToDB(iter):\n",
    "    \n",
    "    client = MongoClient() # connect to MongoDB\n",
    "    db = client.skill_demo_db \n",
    "    collection = db.vic_fire_collection\n",
    "    \n",
    "    climate_data={}\n",
    "    hotspot = {}\n",
    "    later_process = []\n",
    "    \n",
    "    # obtain the newest datetime from mongodb\n",
    "    date_find= collection.find({}).sort('Date',pymongo.DESCENDING).limit(1)\n",
    "    for i in date_find:\n",
    "        new_date = i['Date'] + timedelta(days=1)\n",
    "            \n",
    "    for record in iter:\n",
    "        \n",
    "        data = json.loads(record[1])\n",
    "        #add new attribute as geohash, and do the geohasing for each received data\n",
    "        data['Geohash_5']= gh.encode(float(data['Latitude']),float(data['Longitude']),precision=5)\n",
    "        #update the date\n",
    "        data['Date'] = new_date\n",
    "        if data['Sender_ID'] == 1: # for climate data\n",
    "            climate_data = data\n",
    "            store_hash = data['Geohash_5']\n",
    "        elif data['Sender_ID'] == 2 or data['Sender_ID'] == 3: # for hotspot data\n",
    "            # update the datetime\n",
    "            data['Datetime'] = dt.datetime.combine(new_date,datetime.strptime(data['Created_Time'],\"%X\").time())\n",
    "            # append to late_process\n",
    "            later_process.append(data)\n",
    "      \n",
    "            \n",
    "    if any(climate_data):       \n",
    "        for x in later_process: \n",
    "            if x['Geohash_5'] == store_hash: # if the hotspot and the climate are in same location\n",
    "                if x['Geohash_5'] in hotspot.keys(): # if hotspot already exists, average the temp \n",
    "                    hotspot[x['Geohash_5']]['Surface_Temperature_Celcius'] = (hotspot[x['Geohash_5']]['Surface_Temperature_Celcius'] + x['Surface_Temperature_Celcius'])/2\n",
    "                    hotspot[x['Geohash_5']]['Confidence'] = (hotspot[x['Geohash_5']]['Confidence'] + x['Confidence'])/2\n",
    "                else: # add new hotspot into the dict\n",
    "                    hotspot[x['Geohash_5']] = x\n",
    "        \n",
    "                \n",
    "    if any(hotspot) and any(climate_data): # add the hotspot into the climate, create the data model\n",
    "        climate_data['Hotspot'].append(list(hotspot.values())[0])\n",
    "        \n",
    "        \n",
    "    if any(climate_data): # send the data to MongoDB      \n",
    "        try:\n",
    "            collection.insert(climate_data)\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "        \n",
    "\n",
    "        \n",
    "n_secs = 10 # seconds of interval\n",
    "topic = \"vic_fire_producer\" #topic name\n",
    "\n",
    "conf = SparkConf().setAppName(\"Spark_steaming\").setMaster(\"local[2]\")# 2 threads\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs) \n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'vic_fire_steaming', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
